The following is a digest of the repository "multimodal_agent.git".
This digest is designed to be easily parsed by Large Language Models.

--- SUMMARY ---
Repository: multimodal_agent.git
Files Analyzed: 9
Total Text Size: 43.21 KB
Estimated Tokens (text only): ~8,947

--- DIRECTORY STRUCTURE ---
multimodal_agent.git/
├── files_folder/
│   ├── audio.mp3 [binary]
│   └── business.txt
├── config.py
├── data_processing.py
├── init_db.py
├── main.py
├── qa_chain.py
├── requirements.txt
└── vectorstore_utils.py


--- FILE CONTENTS ---
============================================================
FILE: files_folder/business.txt
============================================================
On the latest episode of BBC Radio 5 Live's Monday Night Club, the panel discuss Arsenal's chances of winning the Premier League this season.

Former Premier League striker, Chris Sutton says he believes Arsenal will win the league comfortably, adding that summer recruit, Viktor Gyokeres, will "eventually start firing and will be a good signing."

He also suggested that Liverpool's drop off and Manchester City's reliance on Erling Haarland will help the Gunners over the line.

Watch the full episode on BBC iPlayer and listen on BBC Sounds

This is your Arsenal page. If you're on the BBC Sport app, make sure you're signed in and hit the bell icon at the top of this page before selecting news notifications.

If you're using a computer, when you're signed in hit 'follow' at the top of this page and you'll see more content about your club when you're on the site.

tony has 4 children

the best player of all time is muller, he won against barcelona 8 times.
in 2013 muller had a big impact in the champions league.

============================================================
FILE: config.py
============================================================
import os
from dotenv import load_dotenv


class Config:
    """Configuration manager for the RAG application."""

    def __init__(self):
        """Initialize configuration by loading environment variables."""
        load_dotenv()

        # Gemini API key
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        if not self.gemini_api_key:
            raise ValueError("GEMINI_API_KEY environment variable not set in .env file")

        # PostgreSQL/pgvector configuration
        self.postgres_host = os.getenv("POSTGRES_HOST", "localhost")
        self.postgres_port = os.getenv("POSTGRES_PORT", "5432")
        self.postgres_db = os.getenv("POSTGRES_DB", "rag_vectorstore")
        self.postgres_user = os.getenv("POSTGRES_USER", "postgres")
        self.postgres_password = os.getenv("POSTGRES_PASSWORD")
        if not self.postgres_password:
            raise ValueError("POSTGRES_PASSWORD environment variable not set in .env file")

        # Vector store collection name
        self.collection_name = os.getenv("COLLECTION_NAME", "document_embeddings")

        # Connection pool settings
        self.postgres_pool_size = int(os.getenv("POSTGRES_POOL_SIZE", "5"))
        self.postgres_max_overflow = int(os.getenv("POSTGRES_MAX_OVERFLOW", "10"))

        # Chunking settings
        self.chunk_size = 300
        self.chunk_overlap = 80

        # Supported file extensions
        self.supported_exts = [".pdf", ".docx", ".txt", ".mp3", ".wav", ".m4a", ".flac", ".ogg"]

    def get_connection_string(self):
        """Get PostgreSQL connection string."""
        return (
            f"postgresql+psycopg://{self.postgres_user}:{self.postgres_password}"
            f"@{self.postgres_host}:{self.postgres_port}/{self.postgres_db}"
        )


============================================================
FILE: data_processing.py
============================================================
import os
import asyncio
from concurrent.futures import ThreadPoolExecutor
import whisper
from colorama import Fore
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import UnstructuredWordDocumentLoader, PyPDFLoader, TextLoader
 
 
class DocumentProcessor:
    """Handles document and audio file processing, transcription, and chunking."""
 
    def __init__(self, config, max_concurrent_files=5):
        """Initialize the document processor.
 
        Args:
            config: Config instance (required)
            max_concurrent_files: Maximum number of files to process concurrently
        """
        self.config = config
        self._whisper_model = None
        self.max_concurrent_files = max_concurrent_files
        self._executor = ThreadPoolExecutor(max_workers=max_concurrent_files)
 
    def get_whisper_model(self):
        """Get or create Whisper model singleton for efficient reuse."""
        if self._whisper_model is None:
            print(Fore.YELLOW + "[INFO] Loading Whisper 'base' model (first time only)...")
            self._whisper_model = whisper.load_model("base")
            print(Fore.GREEN + "[INFO] Whisper model loaded successfully")
        return self._whisper_model
 
    def _transcribe_audio_sync(self, file_path):
        """Internal synchronous transcription method used by async wrapper.
 
        Args:
            file_path: Path to audio file
 
        Returns:
            Transcribed text string
 
        Raises:
            ValueError: If no speech could be transcribed
        """
        print(Fore.YELLOW + f"[INFO] Transcribing audio: {file_path} (this may take a while)...")
        model = self.get_whisper_model()
        result = model.transcribe(file_path, fp16=False)
        transcript = result["text"].strip()
        if not transcript:
            raise ValueError(f"No speech could be transcribed from {file_path}")
        print(Fore.GREEN + f"[INFO] Transcription complete ({len(transcript)} characters)")
        return transcript
 
    async def transcribe_audio_async(self, file_path):
        """Transcribe audio file asynchronously using thread executor.
 
        Args:
            file_path: Path to audio file
 
        Returns:
            Transcribed text string
 
        Raises:
            ValueError: If no speech could be transcribed
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self._executor, self._transcribe_audio_sync, file_path)
 
    async def load_and_chunk_file_async(self, file_path):
        """Load a single document or audio file and split it into chunks (asynchronous).
 
        Args:
            file_path: Path to file to process
 
        Returns:
            List of LangChain Document chunks
 
        Raises:
            FileNotFoundError: If file doesn't exist
            ValueError: If file type is unsupported or no content loaded
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
 
        ext = os.path.splitext(file_path)[1].lower()
        try:
            # Audio files - use async transcription
            if ext in [".mp3", ".wav", ".m4a", ".flac", ".ogg"]:
                transcript = await self.transcribe_audio_async(file_path)
                docs = [Document(page_content=transcript, metadata={"source": file_path})]
 
            # Documents - run in executor to avoid blocking
            else:
                loop = asyncio.get_event_loop()
 
                def load_document():
                    if ext == ".docx":
                        loader = UnstructuredWordDocumentLoader(file_path)
                    elif ext == ".pdf":
                        loader = PyPDFLoader(file_path)
                    elif ext == ".txt":
                        loader = TextLoader(file_path)
                    else:
                        raise ValueError(f"Unsupported file type: {ext}")
                    return loader.load()
 
                docs = await loop.run_in_executor(self._executor, load_document)
 
            if not docs:
                raise ValueError(f"No content loaded from {file_path}")
 
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.config.chunk_size,
                chunk_overlap=self.config.chunk_overlap,
                separators=["\n\n", "\n", ". ", " ", "!", "?"]
            )
            chunks = splitter.split_documents(docs)
            return chunks
 
        except Exception as e:
            print(Fore.RED + f"[ERROR] Failed to process {file_path}: {e}")
            raise
 
    async def process_folder_by_file_async(self, folder_path, vector_manager=None):
        """Scan a folder, process files concurrently, and return chunks grouped by filename.
        Uses incremental processing - only processes new or modified files if vector_manager is provided.
 
        Args:
            folder_path: Path to folder containing documents
            vector_manager: Optional VectorStoreManager for incremental processing
 
        Returns:
            Dict mapping filename -> list of chunks
 
        Raises:
            FileNotFoundError: If folder doesn't exist
            ValueError: If no supported files found
        """
        if not os.path.exists(folder_path):
            raise FileNotFoundError(f"Folder not found: {folder_path}")
 
        files = [f for f in os.listdir(folder_path)
                 if os.path.splitext(f)[1].lower() in self.config.supported_exts]
 
        if not files:
            raise ValueError(f"No supported files found in {folder_path}")
 
        print(Fore.YELLOW + f"[INFO] Scanning folder: {folder_path}")
 
        chunks_by_file = {}
 
        # Determine which files need processing (if vector_manager provided)
        files_to_process = []
        files_skipped = []
 
        if vector_manager:
            for file_name in files:
                file_path = os.path.join(folder_path, file_name)
                needs_update, reason = vector_manager.check_file_needs_update(file_path, file_name)
 
                if needs_update:
                    files_to_process.append(file_name)
                else:
                    files_skipped.append(file_name)
                    # Initialize empty chunks for unchanged files (will be loaded from DB)
                    chunks_by_file[file_name] = []
 
            if files_skipped:
                print(Fore.YELLOW + f"[INFO] Skipping {len(files_skipped)} unchanged files: {', '.join(files_skipped)}")
 
            if not files_to_process:
                print(Fore.GREEN + "[INFO] All files are up to date - no processing needed!")
                return chunks_by_file
        else:
            # No vector manager - process all files
            files_to_process = files
 
        print(Fore.YELLOW + f"[INFO] Processing {len(files_to_process)} files concurrently (max {self.max_concurrent_files} at a time)...")
 
        # Process files with concurrency limit using semaphore
        semaphore = asyncio.Semaphore(self.max_concurrent_files)
 
        async def process_single_file(file_name):
            async with semaphore:
                file_path = os.path.join(folder_path, file_name)
                print(Fore.CYAN + f"[INFO] Processing file: {file_name}")
                try:
                    chunks = await self.load_and_chunk_file_async(file_path)
                    print(Fore.GREEN + f"[INFO] Added {len(chunks)} chunks from {file_name}")
                    return file_name, chunks
                except Exception as e:
                    print(Fore.RED + f"[ERROR] Skipping {file_name}: {e}")
                    return file_name, []
 
        # Process only the files that need processing
        results = await asyncio.gather(*[process_single_file(file_name) for file_name in files_to_process])
 
        # Build the dictionary from results
        for file_name, chunks in results:
            chunks_by_file[file_name] = chunks
 
        total_chunks = sum(len(chunks) for chunks in chunks_by_file.values())
        print(Fore.GREEN + f"[INFO] Finished processing {len(files_to_process)} files. Total chunks: {total_chunks}")
        return chunks_by_file
 

============================================================
FILE: init_db.py
============================================================
"""
Database initialization script for pgvector RAG system.

This script:
1. Creates the PostgreSQL database if it doesn't exist
2. Enables the pgvector extension
3. Verifies the setup

Usage:
    python init_db.py
"""

import sys
from sqlalchemy import create_engine, text
from sqlalchemy.exc import OperationalError, ProgrammingError
from colorama import Fore, Style
from config import (
    POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB,
    POSTGRES_USER, POSTGRES_PASSWORD
)


def create_database():
    """Create the database if it doesn't exist."""
    # Connect to default 'postgres' database to create our target database
    admin_connection_string = (
        f"postgresql+psycopg://{POSTGRES_USER}:{POSTGRES_PASSWORD}"
        f"@{POSTGRES_HOST}:{POSTGRES_PORT}/postgres"
    )

    try:
        engine = create_engine(admin_connection_string, isolation_level="AUTOCOMMIT")
        with engine.connect() as conn:
            # Check if database exists
            result = conn.execute(
                text("SELECT 1 FROM pg_database WHERE datname = :dbname"),
                {"dbname": POSTGRES_DB}
            )

            if result.fetchone() is None:
                print(Fore.YELLOW + f"[INFO] Creating database '{POSTGRES_DB}'...")
                conn.execute(text(f'CREATE DATABASE "{POSTGRES_DB}"'))
                print(Fore.GREEN + f"[INFO] Database '{POSTGRES_DB}' created successfully!")
            else:
                print(Fore.GREEN + f"[INFO] Database '{POSTGRES_DB}' already exists.")

        engine.dispose()
        return True

    except OperationalError as e:
        print(Fore.RED + f"[ERROR] Could not connect to PostgreSQL: {e}")
        print(Fore.YELLOW + "\nPlease ensure:")
        print("  1. PostgreSQL is running")
        print(f"  2. User '{POSTGRES_USER}' exists and has proper permissions")
        print(f"  3. Connection details in .env are correct")
        return False
    except Exception as e:
        print(Fore.RED + f"[ERROR] Failed to create database: {e}")
        return False


def enable_pgvector():
    """Enable pgvector extension in the target database."""
    connection_string = (
        f"postgresql+psycopg://{POSTGRES_USER}:{POSTGRES_PASSWORD}"
        f"@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"
    )

    try:
        engine = create_engine(connection_string)
        with engine.connect() as conn:
            print(Fore.YELLOW + "[INFO] Enabling pgvector extension...")
            conn.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
            conn.commit()
            print(Fore.GREEN + "[INFO] pgvector extension enabled successfully!")

            # Verify installation
            result = conn.execute(text("SELECT extversion FROM pg_extension WHERE extname = 'vector'"))
            version = result.fetchone()
            if version:
                print(Fore.GREEN + f"[INFO] pgvector version: {version[0]}")

        engine.dispose()
        return True

    except ProgrammingError as e:
        print(Fore.RED + f"[ERROR] pgvector extension not available: {e}")
        print(Fore.YELLOW + "\nPlease install pgvector:")
        print("  - Ubuntu/Debian: sudo apt install postgresql-pgvector")
        print("  - macOS: brew install pgvector")
        print("  - Windows: Download from https://github.com/pgvector/pgvector/releases")
        print("  - Docker: Use postgres image with pgvector (e.g., pgvector/pgvector:pg16)")
        return False
    except Exception as e:
        print(Fore.RED + f"[ERROR] Failed to enable pgvector: {e}")
        return False


def verify_setup():
    """Verify the database setup is complete."""
    connection_string = (
        f"postgresql+psycopg://{POSTGRES_USER}:{POSTGRES_PASSWORD}"
        f"@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"
    )

    try:
        engine = create_engine(connection_string)
        with engine.connect() as conn:
            # Test vector operations
            conn.execute(text("SELECT '[1,2,3]'::vector"))
            print(Fore.GREEN + "[INFO] Vector operations working correctly!")

        engine.dispose()
        return True

    except Exception as e:
        print(Fore.RED + f"[ERROR] Setup verification failed: {e}")
        return False


def main():
    """Main initialization flow."""
    print(Fore.CYAN + "=" * 60)
    print(Fore.CYAN + "PostgreSQL + pgvector Database Initialization")
    print(Fore.CYAN + "=" * 60 + "\n")

    print(Fore.YELLOW + f"Database: {POSTGRES_DB}")
    print(Fore.YELLOW + f"Host: {POSTGRES_HOST}:{POSTGRES_PORT}")
    print(Fore.YELLOW + f"User: {POSTGRES_USER}\n")

    # Step 1: Create database
    if not create_database():
        sys.exit(1)

    # Step 2: Enable pgvector
    if not enable_pgvector():
        sys.exit(1)

    # Step 3: Verify setup
    if not verify_setup():
        sys.exit(1)

    print(Fore.GREEN + "\n" + "=" * 60)
    print(Fore.GREEN + "Database initialization complete!")
    print(Fore.GREEN + "You can now run: python main.py")
    print(Fore.GREEN + "=" * 60)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print(Fore.YELLOW + "\n\nInitialization cancelled by user.")
        sys.exit(1)
    except Exception as e:
        print(Fore.RED + f"\n[ERROR] Unexpected error: {e}")
        sys.exit(1)


============================================================
FILE: main.py
============================================================
import asyncio
from config import Config
from vectorstore_utils import VectorStoreManager
from data_processing import DocumentProcessor
from qa_chain import QASystem
from colorama import Fore, Style
import sys
 
 
class RAGApplication:
    """Main RAG application that orchestrates document processing and Q&A."""
 
    def __init__(self, folder_path="files_folder", max_concurrent_files=5, max_concurrent_stores=3):
        """Initialize the RAG application.
 
        Args:
            folder_path: Path to folder containing documents
            max_concurrent_files: Maximum number of files to process concurrently
            max_concurrent_stores: Maximum number of vectorstores to create concurrently
        """
        self.folder_path = folder_path
        self.config = Config()
 
        # Initialize components
        self.doc_processor = DocumentProcessor(self.config, max_concurrent_files=max_concurrent_files)
        self.vector_manager = VectorStoreManager(self.config, max_concurrent_stores=max_concurrent_stores)
        self.qa_system = None
 
    async def initialize_async(self):
        """Initialize the application by processing documents and creating vector stores (asynchronous)."""
        print(Fore.YELLOW + "[INFO] Processing files and creating per-file pgvector stores (async mode)...")
 
        # Initialize vectorstore (creates tables)
        self.vector_manager.init_vectorstore()
 
        # Process documents concurrently (with incremental updates - checks hashes first!)
        chunks_by_file = await self.doc_processor.process_folder_by_file_async(
            self.folder_path,
            vector_manager=self.vector_manager
        )
 
        # Create vector stores concurrently (only for new/modified files)
        vectorstores = await self.vector_manager.create_per_file_vectorstores_async(chunks_by_file, self.folder_path)
        print(Fore.GREEN + "[INFO] pgvector stores ready in PostgreSQL!")
 
        # Initialize QA system
        self.qa_system = QASystem(self.vector_manager, self.config)
        self.qa_system.create_qa_chain()
 
        return vectorstores
 
    def display_indexed_files(self):
        """Display the list of indexed files."""
        available_files = self.vector_manager.get_available_files()
        print(Fore.GREEN + "\nDocument/Audio Q&A system ready!")
        print(Fore.CYAN + f"\nIndexed files ({len(available_files)}):")
 
        for i, filename in enumerate(available_files, 1):
            # Identify file type
            ext = filename.split('.')[-1].lower()
            if ext in ['mp3', 'wav', 'm4a', 'flac', 'ogg']:
                file_type = "Audio"
            elif ext == 'pdf':
                file_type = "PDF"
            elif ext == 'docx':
                file_type = "Word"
            elif ext == 'txt':
                file_type = "Text"
            else:
                file_type = "Document"
            print(f"  {i}. {filename} ({file_type})")
 
        print(Fore.YELLOW + "\nTip: Be specific about which file you're asking about (e.g., 'What does the audio say about...?')")
        print(Fore.YELLOW + "     The system will prioritize information from the file you mention.\n")
 
    def run_interactive_loop(self):
        """Run the interactive Q&A loop."""
        while True:
            query = input(Style.RESET_ALL + "Ask a question (or type 'exit'): ")
 
            if query.lower() in ['exit', 'quit', 'q']:
                print("Goodbye!")
                break
 
            if not query.strip():
                continue
 
            try:
                # Use streaming for real-time response display
                print(Fore.GREEN + "\nAnswer: ", end="", flush=True)
 
                full_response = ""
                for chunk in self.qa_system.ask(query, stream=True):
                    print(chunk, end="", flush=True)
                    full_response += chunk
                    sys.stdout.flush()  # Ensure immediate output
 
                print("\n")  # New line after complete response
 
                # Update history
                self.qa_system.update_history(query, full_response)
 
            except Exception as e:
                print(Fore.RED + f"\nError processing query: {e}")
 
    def run(self):
        """Run the complete application workflow."""
        try:
            # Run async initialization
            asyncio.run(self.initialize_async())
 
            self.display_indexed_files()
            self.run_interactive_loop()
        except Exception as e:
            print(Fore.RED + f"Application error: {e}")
 
 
if __name__ == "__main__":
    app = RAGApplication(folder_path="files_folder")
    app.run()
 

============================================================
FILE: qa_chain.py
============================================================
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
import os
import re


class QASystem:
    """Manages the question-answering system using LLM and vector stores."""

    def __init__(self, vector_store_manager, config):
        """Initialize the QA system.

        Args:
            vector_store_manager: VectorStoreManager instance
            config: Config instance (required)
        """
        self.config = config
        self.vector_store_manager = vector_store_manager
        self.llm = None
        self.qa_chain = None
        self.chat_history = []
        self._initialize_llm()

    def _initialize_llm(self):
        """Initialize the LLM (Gemini)."""
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            temperature=0.2,
            google_api_key=self.config.gemini_api_key,
            streaming=True
        )

    def format_docs(self, docs):
        """Format documents with source file information.

        Args:
            docs: List of Document objects

        Returns:
            Formatted string with source information
        """
        formatted = []
        for doc in docs:
            source = doc.metadata.get("source", "unknown")
            source_name = source.split("\\")[-1].split("/")[-1]  # Get filename only
            formatted.append(f"[From: {source_name}]\n{doc.page_content}")
        return "\n\n".join(formatted)

    def detect_file_mention(self, question, available_files):
        """Detect if user is asking about a specific file or file type.

        Args:
            question: User's question
            available_files: List of available filenames

        Returns:
            List of mentioned filenames
        """
        question_lower = question.lower()

        # File type mentions with more variations
        file_type_map = {
            'audio': ['.mp3', '.wav', '.m4a', '.flac', '.ogg'],
            'mp3': ['.mp3'],
            'wav': ['.wav'],
            'pdf': ['.pdf'],
            'document': ['.docx'],
            'text': ['.txt'],
            'word': ['.docx'],
            'docx': ['.docx']
        }

        mentioned_files = []

        # Check for specific filename mentions
        for filename in available_files:
            filename_lower = filename.lower()
            # Check if filename (without extension) is mentioned
            name_without_ext = os.path.splitext(filename_lower)[0]
            if name_without_ext in question_lower or filename_lower in question_lower:
                mentioned_files.append(filename)

        # Check for file type mentions (e.g., "the audio", "the PDF", "mp3 file")
        if not mentioned_files:
            for file_type, extensions in file_type_map.items():
                # Check for various patterns like "audio file", "the mp3", "mp3 file", etc.
                if file_type in question_lower or f"{file_type} file" in question_lower:
                    for filename in available_files:
                        if any(filename.lower().endswith(ext) for ext in extensions):
                            mentioned_files.append(filename)

        return mentioned_files

    def retrieve_from_vectorstores(self, question, k=8):
        """Retrieve documents using separate pgvector vectorstores per file.

        Args:
            question: User's question
            k: Number of documents to retrieve

        Returns:
            List of retrieved documents
        """
        vectorstores = self.vector_store_manager.vectorstores
        available_files = self.vector_store_manager.get_available_files()
        mentioned_files = self.detect_file_mention(question, available_files)

        if mentioned_files:
            # Query ONLY the mentioned file's vectorstore
            all_docs = []
            for filename in mentioned_files:
                if filename in vectorstores:
                    retriever = vectorstores[filename].as_retriever(
                        search_type="similarity",
                        search_kwargs={"k": k}
                    )
                    docs = retriever.invoke(question)
                    all_docs.extend(docs)

            # Return documents from mentioned files only
            if all_docs:
                return all_docs[:k]

        # No specific file mentioned: search across all vectorstores and aggregate results
        all_docs = []
        docs_per_file = max(1, k // len(vectorstores))  # Distribute k across files

        for filename, vectorstore in vectorstores.items():
            retriever = vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": docs_per_file}
            )
            docs = retriever.invoke(question)
            all_docs.extend(docs)

        return all_docs[:k]

    def create_qa_chain(self):
        """Create QA chain using pgvector vectorstores.

        Returns:
            LangChain QA chain
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a helpful assistant answering questions about documents and audio transcripts.
            Use the following context from the documents/transcripts and the conversation history to answer questions.

            IMPORTANT: Each piece of context shows which file it came from using [From: filename].
            - Pay close attention to which file the user is asking about
            - When answering, prioritize information from the relevant file
            - If the user asks about a specific file (e.g., "the audio", "the PDF", "business.txt"), focus on context from that file
            - If multiple files contain relevant information, mention which file each piece of information comes from

            If you don't know the answer based on the context, just say that you don't know.

            Context from documents: {context}"""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}")
        ])

        self.qa_chain = (
            {
                "context": lambda x: self.format_docs(
                    self.retrieve_from_vectorstores(x["question"], k=8)
                ),
                "question": lambda x: x["question"],
                "chat_history": lambda x: x["chat_history"]
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return self.qa_chain

    def ask(self, question, stream=True):
        """Ask a question and get an answer.

        Args:
            question: User's question
            stream: Whether to stream the response

        Returns:
            If stream=True: Generator yielding response chunks
            If stream=False: Complete response string
        """
        if self.qa_chain is None:
            self.create_qa_chain()

        if stream:
            return self.qa_chain.stream({"question": question, "chat_history": self.chat_history})
        else:
            response = self.qa_chain.invoke({"question": question, "chat_history": self.chat_history})
            return response

    def update_history(self, question, answer):
        """Update chat history with the latest Q&A pair.

        Args:
            question: User's question
            answer: Assistant's answer
        """
        self.chat_history.append(HumanMessage(content=question))
        self.chat_history.append(AIMessage(content=answer))

        # Keep last 5 Q&A pairs (10 messages)
        if len(self.chat_history) > 10:
            self.chat_history = self.chat_history[-10:]

    def clear_history(self):
        """Clear the chat history."""
        self.chat_history = []


============================================================
FILE: requirements.txt
============================================================
# LangChain core and integrations
langchain>=0.1.0
langchain-community>=0.0.20
langchain-google-genai>=1.0.0
langchain-huggingface>=0.0.1
langchain-core>=0.1.0
langchain-text-splitters>=0.0.1

# Vector store (pgvector)
langchain-postgres>=0.0.12
psycopg[binary]>=3.1.0
sqlalchemy>=2.0.0

# Audio processing
openai-whisper>=20231117

# Embeddings
sentence-transformers>=2.2.0

# Document loaders
unstructured>=0.10.0
pypdf>=3.17.0
python-docx>=1.1.0

# Utilities
colorama>=0.4.6
python-dotenv>=1.0.0

# Async support (built-in to Python 3.7+)
# asyncio - concurrent file processing
# concurrent.futures - ThreadPoolExecutor for CPU-bound tasks

# Optional: For GPU acceleration (comment out if using CPU only)
# torch>=2.0.0


============================================================
FILE: vectorstore_utils.py
============================================================
import asyncio
from concurrent.futures import ThreadPoolExecutor
from langchain_postgres import PGVector
from langchain_huggingface import HuggingFaceEmbeddings
from sqlalchemy import create_engine, text, MetaData, Table, Column, String, Integer, DateTime
from sqlalchemy.pool import QueuePool
import os
import hashlib
from datetime import datetime
 
 
class VectorStoreManager:
    """Manages pgvector vector stores, embeddings, and database connections."""
 
    def __init__(self, config, max_concurrent_stores=3):
        """Initialize the vector store manager.
 
        Args:
            config: Config instance (required)
            max_concurrent_stores: Maximum number of vectorstores to create concurrently
        """
        self.config = config
        self._connection_string = None
        self._embeddings = None
        self._engine = None
        self._metadata = None
        self.vectorstores = {}
        self.max_concurrent_stores = max_concurrent_stores
        self._executor = ThreadPoolExecutor(max_workers=max_concurrent_stores)
 
    def get_connection_string(self):
        """Get PostgreSQL connection string (cached)."""
        if self._connection_string is None:
            self._connection_string = self.config.get_connection_string()
        return self._connection_string
 
    def get_embeddings(self):
        """Get or create embeddings model singleton for efficient reuse."""
        if self._embeddings is None:
            self._embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
        return self._embeddings
 
    def get_engine(self):
        """Get or create SQLAlchemy engine with connection pooling."""
        if self._engine is None:
            self._engine = create_engine(
                self.get_connection_string(),
                poolclass=QueuePool,
                pool_size=self.config.postgres_pool_size,
                max_overflow=self.config.postgres_max_overflow,
                pool_pre_ping=True,  # Verify connections before using
                echo=False
            )
        return self._engine
 
    def init_vectorstore(self):
        """Initialize pgvector extension and create necessary tables."""
        engine = self.get_engine()
        with engine.connect() as conn:
            # Enable pgvector extension
            conn.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
            conn.commit()
            print(f"[INFO] pgvector extension enabled on database '{self.config.postgres_db}'")
 
        # Create metadata tracking table
        self._create_metadata_table()
 
    def _get_metadata(self):
        """Get or create SQLAlchemy MetaData object (singleton)."""
        if self._metadata is None:
            self._metadata = MetaData()
        return self._metadata
 
    def _create_metadata_table(self):
        """Create a table to track file hashes and modification times."""
        engine = self.get_engine()
        metadata = self._get_metadata()
 
        # Define file tracking table
        file_tracking = Table(
            'rag_file_tracking',
            metadata,
            Column('filename', String(255), primary_key=True),
            Column('file_hash', String(64), nullable=False),
            Column('chunk_count', Integer, nullable=False),
            Column('last_processed', DateTime, nullable=False),
            extend_existing=True
        )
 
        # Create table if not exists
        metadata.create_all(engine, checkfirst=True)
 
    def _compute_file_hash(self, file_path):
        """Compute SHA-256 hash of a file.
 
        Args:
            file_path: Path to file
 
        Returns:
            Hexadecimal hash string
        """
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            # Read file in chunks to handle large files
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
 
    def _get_stored_file_hash(self, filename):
        """Get stored hash for a file from database.
 
        Args:
            filename: Name of file
 
        Returns:
            Hash string or None if file not tracked
        """
        engine = self.get_engine()
        with engine.connect() as conn:
            result = conn.execute(
                text("SELECT file_hash FROM rag_file_tracking WHERE filename = :filename"),
                {"filename": filename}
            )
            row = result.fetchone()
            return row[0] if row else None
 
    def _update_file_tracking(self, filename, file_hash, chunk_count):
        """Update or insert file tracking record.
 
        Args:
            filename: Name of file
            file_hash: SHA-256 hash of file
            chunk_count: Number of chunks created
        """
        engine = self.get_engine()
        with engine.connect() as conn:
            # Upsert (PostgreSQL syntax)
            conn.execute(
                text("""
                    INSERT INTO rag_file_tracking (filename, file_hash, chunk_count, last_processed)
                    VALUES (:filename, :file_hash, :chunk_count, :last_processed)
                    ON CONFLICT (filename)
                    DO UPDATE SET
                        file_hash = :file_hash,
                        chunk_count = :chunk_count,
                        last_processed = :last_processed
                """),
                {
                    "filename": filename,
                    "file_hash": file_hash,
                    "chunk_count": chunk_count,
                    "last_processed": datetime.now()
                }
            )
            conn.commit()
 
    def check_file_needs_update(self, file_path, filename):
        """Check if a file needs to be reprocessed based on hash.
 
        Args:
            file_path: Full path to file
            filename: Name of file
 
        Returns:
            Tuple of (needs_update: bool, reason: str)
        """
        if not os.path.exists(file_path):
            return False, "File not found"
 
        # Compute current hash
        current_hash = self._compute_file_hash(file_path)
 
        # Get stored hash
        stored_hash = self._get_stored_file_hash(filename)
 
        if stored_hash is None:
            return True, "New file"
        elif current_hash != stored_hash:
            return True, "File modified"
        else:
            return False, "File unchanged"
 
    def sanitize_collection_name(self, filename):
        """Convert filename to valid PostgreSQL table name.
 
        Args:
            filename: Original filename
 
        Returns:
            Sanitized collection name
        """
        # Remove extension and replace invalid chars with underscores
        name = os.path.splitext(filename)[0]
        name = ''.join(c if c.isalnum() else '_' for c in name)
        return name.lower()[:50]  # Limit length
 
    async def create_per_file_vectorstores_async(self, chunks_by_file, folder_path):
        """Create pgvector vectorstores with per-file metadata (asynchronous with concurrency control).
        Uses incremental processing - only creates/updates vectorstores for files with chunks.
 
        Args:
            chunks_by_file: Dict mapping filename -> list of chunks (empty list = unchanged file)
            folder_path: Path to folder containing files (for hash checking)
 
        Returns:
            Dict mapping filename -> PGVector vectorstore instance
        """
        self.init_vectorstore()
        embeddings = self.get_embeddings()
        connection_string = self.get_connection_string()
 
        # Load existing vectorstores for files with empty chunks (unchanged files)
        self._load_existing_vectorstores(chunks_by_file)
 
        # Determine which files need vectorstore creation (files with chunks)
        files_to_process = []
 
        for filename, chunks in chunks_by_file.items():
            if chunks:  # Only process files that have chunks (newly processed files)
                file_path = os.path.join(folder_path, filename)
                files_to_process.append((filename, chunks, file_path))
 
        if not files_to_process:
            print("[INFO] No new vectorstores to create - all loaded from database!")
            return self.vectorstores
 
        print(f"[INFO] Creating {len(files_to_process)} vectorstores concurrently (max {self.max_concurrent_stores} at a time)...")
 
        # Semaphore to limit concurrent vectorstore creation
        semaphore = asyncio.Semaphore(self.max_concurrent_stores)
 
        async def create_single_vectorstore(filename, chunks, file_path):
            async with semaphore:
                loop = asyncio.get_event_loop()
 
                def create_store():
                    # Add filename to each chunk's metadata
                    for chunk in chunks:
                        chunk.metadata["filename"] = filename
 
                    # Create or use existing collection with filename as collection suffix
                    collection_name = f"{self.config.collection_name}_{self.sanitize_collection_name(filename)}"
 
                    # Create PGVector store from documents with MetaData fix
                    vectorstore = PGVector.from_documents(
                        documents=chunks,
                        embedding=embeddings,
                        collection_name=collection_name,
                        connection=connection_string,
                        pre_delete_collection=True,  # Clear existing data for this file
                        use_jsonb=True  # Use JSONB for metadata (better performance)
                    )
 
                    # Update file tracking with hash
                    file_hash = self._compute_file_hash(file_path)
                    self._update_file_tracking(filename, file_hash, len(chunks))
 
                    print(f"[INFO] Created pgvector store for '{filename}' ({len(chunks)} chunks)")
                    return filename, vectorstore
 
                return await loop.run_in_executor(self._executor, create_store)
 
        # Create all vectorstores concurrently
        results = await asyncio.gather(*[create_single_vectorstore(filename, chunks, file_path)
                                        for filename, chunks, file_path in files_to_process])
 
        # Build vectorstores dictionary
        for filename, vectorstore in results:
            self.vectorstores[filename] = vectorstore
 
        return self.vectorstores
 
    def _load_existing_vectorstores(self, chunks_by_file):
        """Load existing vectorstores for files that haven't changed.
 
        Args:
            chunks_by_file: Dict mapping filename -> list of chunks
        """
        embeddings = self.get_embeddings()
        connection_string = self.get_connection_string()
        engine = self.get_engine()
 
        for filename in chunks_by_file.keys():
            # Check if collection exists in database
            collection_name = f"{self.config.collection_name}_{self.sanitize_collection_name(filename)}"
 
            try:
                # Verify collection table exists in database
                with engine.connect() as conn:
                    result = conn.execute(
                        text("""
                            SELECT EXISTS (
                                SELECT FROM information_schema.tables
                                WHERE table_schema = 'public'
                                AND table_name = :table_name
                            )
                        """),
                        {"table_name": f"langchain_pg_collection"}
                    )
                    tables_exist = result.scalar()
 
                    if not tables_exist:
                        # Tables not created yet
                        continue
 
                    # Check if this specific collection exists
                    result = conn.execute(
                        text("""
                            SELECT EXISTS (
                                SELECT 1 FROM langchain_pg_collection
                                WHERE name = :collection_name
                            )
                        """),
                        {"collection_name": collection_name}
                    )
                    collection_exists = result.scalar()
 
                if collection_exists:
                    # Load existing vectorstore
                    vectorstore = PGVector(
                        embeddings=embeddings,
                        collection_name=collection_name,
                        connection=connection_string,
                        use_jsonb=True
                    )
                    self.vectorstores[filename] = vectorstore
                    print(f"[INFO] Loaded existing vectorstore for '{filename}'")
 
            except Exception as e:
                # Collection doesn't exist or is invalid - will be created later
                print(f"[DEBUG] Could not load vectorstore for '{filename}': {e}")
                pass
 
    def get_available_files(self):
        """Get list of available files from vectorstores.
 
        Returns:
            List of filenames
        """
        return sorted(list(self.vectorstores.keys()))